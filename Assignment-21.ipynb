{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "Ans:-\n",
    "    Web scraping is an automatic method to obtain large amounts of data from websites.\n",
    "    Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. \n",
    "    There are many different ways to perform web scraping to obtain data from websites.\n",
    "    These include using online services, particular API’s or even creating your code for web scraping from scratch. \n",
    "    Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have API’s that allow you to access their data in a structured format. \n",
    "    This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced.\n",
    "    In that situation, it’s best to use Web Scraping to scrape the website for data.\n",
    "    \n",
    "    Web scraping is used in a variety of digital businesses that rely on data harvesting.\n",
    "    Legitimate use cases include: Search engine bots crawling a site, analyzing its content and then ranking it. \n",
    "    Price comparison sites deploying bots to auto-fetch prices and product descriptions for allied seller websites.\n",
    "    \n",
    "    \n",
    "1. Price Monitoring\n",
    "Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.\n",
    "\n",
    "2. Market Research\n",
    "Web scraping can be used for market research by companies. High-quality web scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. \n",
    "\n",
    "3. News Monitoring\n",
    "Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ffe237",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the different methods used for Web Scraping?\n",
    "Ans:-\n",
    "    Google Sheets is a popular tool for data scraping.\n",
    "    Scarpers can use the IMPORTXML function in Sheets to scrape from a website, which is useful if they want to extract a specific pattern or data from the website.\n",
    "    This command also makes it possible to check if a website can be scraped or is protected.\n",
    "    \n",
    "1.Collect business intelligence to inform web content\n",
    "2.Determine prices for travel booking or comparison sites\n",
    "3.Find sales leads or conduct market research via public data sources\n",
    "4.Send product data from eCommerce sites to online shopping platforms like Google Shopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "ans:-\n",
    "    Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup).\n",
    "    It creates a parse tree for parsed pages that can be used to extract data from HTML which is useful for web scraping\n",
    "    \n",
    "    Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files.\n",
    "    It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8cb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Why is flask used in this Web Scraping project?\n",
    "Ans:-\n",
    "    Flask is a lightweight framework to build websites.\n",
    "    We'll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "    The requests module allows us to send http requests to the website we want to scrape. \n",
    "    The first line imports the Flask class and the render_template method from the flask library.\n",
    "    \n",
    "    Flask is an API of Python that allows us to build up web-applications. \n",
    "    It was developed by Armin Ronacher. Flask’s framework is more explicit than Django’s framework and is also easier to learn because it has less base code to implement a simple web-Application. \n",
    "    A Web-Application Framework or Web Framework is the collection of modules and libraries that helps the developer to write applications without writing the low-level codes such as protocols, thread management, etc. \n",
    "    Flask is based on WSGI(Web Server Gateway Interface) toolkit and Jinja2 template engine.\n",
    "    \n",
    "    The Flask application is started by calling the run() function. The method should be restarted manually for any change in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "Ans:-\n",
    "1. Amazon EC2 (Elastic Compute Cloud)\n",
    "EC2 is a cloud platform provided by Amazon that offers secure, and resizable compute capacity.\n",
    "Its purpose is to enable easy access and usability to developers for web-scale cloud computing, while allowing for total control of your compute resources.\n",
    "    Deploy applications rapidly without the need for investing in hardware upfront; all the while able to launch virtual servers as-needed and at scale.\n",
    "    \n",
    "2. Amazon S3 (Simple Storage Service)\n",
    "Amazon S3, at its core, facilitates object storage, providing leading scalability, data availability, security, and performance.\n",
    "Businesses of vast sizes can leverage S3 for storage and protect large sums of data for various use cases, such as websites, applications, backup, and more.\n",
    "     Amazon S3’s intuitive management features enable the frictionless organization of data and configurable access controls.\n",
    "\n",
    "    \n",
    "3. Amazon Lambda\n",
    "Lambda permits you to run code without owning or managing servers.\n",
    "Users only pay for the compute time consumed.\n",
    "Operate code for nearly any application or backend utility without administration. Users just upload the code, and Lambda does the rest, which provides precise software scaling and extensive availability.    \n",
    "    \n",
    "4. Amazon Cognito\n",
    "AWS Cognito administers a control access dashboard for on-boarding users through sign-up, and sign-in features to their web and mobile apps. AWS Cognito scales to millions of users and offers sign-in support with social identity providers including Facebook, Google, and Amazon, along with enterprise identity providers via SAML 2.0.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
